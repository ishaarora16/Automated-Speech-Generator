{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using Bidirectional LSTM and Doc2Vec models\n",
    "\n",
    "\n",
    "Text generated using a LSTM ususally  provide a taste of unachievement. Generated sentences seems quite right, whith correct grammar and syntax, as if the neural network was understanding correctly the structure of a sentence. But the whole new text does not have great sense. If it is not complete nosense. \n",
    "\n",
    "This problem could come from the approach itself, using only LSTM to generate text word by word. \n",
    "\n",
    "In this method will use LTSM network to generate sequences of words. However we try to go further than a classic LSTM neural network and I will use an additional neural network (LSTM again), to select the best phrases.\n",
    "\n",
    "The approach we use involves the following steps:\n",
    " 1. **how to train a neural network to generate sentences** (i.e. sequences of words), based on existing speeches. We used a bidirectional LSTM Architecture to perform that.\n",
    " 2. **how to train a neural network to select the best next sentence for given paragraph** (i.e. a sequence of sentences). We will  use  use a bidirectional LSTM archicture, in addition to a Doc2Vec model of the targeted speeches.\n",
    "\n",
    "\n",
    "## 1. a Neural Network for Generating Sentences\n",
    "\n",
    "The first step is to generate sentences in the style of a given personality.\n",
    "LSTM (Long Short Term Memory) are very good for analysing sequences of values and predicting the next values from them. For example, LSTM could be a very good choice if we want to predict the very next point of a given time series.\n",
    "\n",
    "Talking about sentences and texts ; phrases (sentences) are basically sequences of words. So,we can assume that LSTM could be usefull to generate the next word of a given sentence.\n",
    "\n",
    "\n",
    "### 1.1.1. Process\n",
    "\n",
    "In order to do that, first, we build a dictionary containing all words from the novels we want to use.\n",
    "\n",
    " 1. read the data (the speeches we want to use),\n",
    " 1. create the dictionnary of words,\n",
    " 2. create the list of sentences,\n",
    " 3. create the neural network,\n",
    " 4. train the neural network,\n",
    " 5. generate new sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tulika\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Input, Flatten, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import categorical_accuracy\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "from six.moves import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have raw text and a lot of things have to be done to use them: split them in words list, etc.\n",
    "In order to do that, I use the spacy library which is incredible to deal with texts. For this exercice, I will only use very few options from spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy, and french model\n",
    "#import spacy\n",
    "#nlp = spacy.load('fr')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = 'data/Artistes_et_Phalanges-David_Campion'# data directory containing input.txt\n",
    "save_dir = 'save' # directory to store models\n",
    "seq_length = 30 # sequence length\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_list = [\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\"312\",\"313\",\"314\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\"410\",\"411\",\"412\"]\n",
    "\n",
    "vocab_file = \"words_vocab.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data\n",
    "\n",
    "Create a list of words from raw text. We use spacy library, with a specific function to retrieve only lower character of the words and remove carriage returns (\\n).\n",
    "\n",
    "We are doing that because we want to reduce the number of potential words in  dictionnary, and we assume we do not have to avoid capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "\n",
    "input_file =  \"speech.txt\"\n",
    "#read data\n",
    "with codecs.open(input_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "#create sentences\n",
    "doc = nlp(data)\n",
    "wl = create_wordlist(doc)\n",
    "wordlist = wordlist + wl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionnary\n",
    "\n",
    "The first step is to create the dictionnary, it means, the list of all words contained in texts. For each word, we will assign an index to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  1142\n"
     ]
    }
   ],
   "source": [
    "# count the number of words\n",
    "word_counts = collections.Counter(wordlist)\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "#size of the vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n",
    "#save the words and vocabulary\n",
    "with open(os.path.join(vocab_file), 'wb') as f:\n",
    "    cPickle.dump((words, vocab, vocabulary_inv), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create sequences\n",
    "Now, we have to create the input data for our LSTM. We create two lists:\n",
    " - **sequences**: this list will contain the sequences of words used to train the model,\n",
    " - **next_words**: this list will contain the next words for each sequences of the **sequences** list.\n",
    " \n",
    "We assume the seq_length = 30.\n",
    "\n",
    "So, to create the first sequence of words, we take the 30th first words in the **wordlist** list. The word 31 is the next word of this first sequence, and is added to the **next_words** list.\n",
    "\n",
    "Then we jump by a step of 1 in the list of words, to create the second sequence of words and retrieve the second \"next word\".\n",
    "\n",
    "We iterate this task until the end of the list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 5480\n"
     ]
    }
   ],
   "source": [
    "#create sequences\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(wordlist) - seq_length, sequences_step):\n",
    "    sequences.append(wordlist[i: i + seq_length])\n",
    "    next_words.append(wordlist[i + seq_length])\n",
    "\n",
    "print('nb sequences:', len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we iterate over the whole list of words, we create 5480 sequences of words, and retrieve, for each of them, the next word to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), seq_length, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), vocab_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        X[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the neural network.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(seq_length, vocab_size):\n",
    "    print('Build LSTM model.')\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(seq_length, vocab_size)))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_size = 256 # size of RNN\n",
    "batch_size = 32 # minibatch size\n",
    "seq_length = 30 # sequence length\n",
    "num_epochs = 50 # number of epochs\n",
    "learning_rate = 0.001 #learning rate\n",
    "sequences_step = 1 #step to create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 512)               2865152   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1142)              585846    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1142)              0         \n",
      "=================================================================\n",
      "Total params: 3,450,998\n",
      "Trainable params: 3,450,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "md = bidirectional_lstm_model(seq_length, vocab_size)\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough speech, we train the model now. We shuffle the training set and extract 10% of it as validation sample. We simply run :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2740 samples, validate on 2740 samples\n",
      "Epoch 1/50\n",
      "2740/2740 [==============================] - ETA: 7:57 - loss: 7.0391 - categorical_accuracy: 0.0000e+ - ETA: 4:02 - loss: 7.0387 - categorical_accuracy: 0.0000e+ - ETA: 2:44 - loss: 7.0374 - categorical_accuracy: 0.0000e+ - ETA: 2:04 - loss: 7.0374 - categorical_accuracy: 0.0000e+ - ETA: 1:41 - loss: 7.0341 - categorical_accuracy: 0.0063   - ETA: 1:25 - loss: 7.0323 - categorical_accuracy: 0.01 - ETA: 1:14 - loss: 7.0290 - categorical_accuracy: 0.01 - ETA: 1:05 - loss: 7.0257 - categorical_accuracy: 0.02 - ETA: 58s - loss: 7.0223 - categorical_accuracy: 0.0208 - ETA: 53s - loss: 7.0198 - categorical_accuracy: 0.021 - ETA: 48s - loss: 7.0173 - categorical_accuracy: 0.019 - ETA: 45s - loss: 7.0129 - categorical_accuracy: 0.018 - ETA: 41s - loss: 7.0084 - categorical_accuracy: 0.016 - ETA: 39s - loss: 6.9925 - categorical_accuracy: 0.017 - ETA: 36s - loss: 6.9789 - categorical_accuracy: 0.016 - ETA: 34s - loss: 6.9421 - categorical_accuracy: 0.019 - ETA: 32s - loss: 6.9099 - categorical_accuracy: 0.022 - ETA: 30s - loss: 6.8727 - categorical_accuracy: 0.026 - ETA: 29s - loss: 6.8661 - categorical_accuracy: 0.026 - ETA: 28s - loss: 6.8380 - categorical_accuracy: 0.026 - ETA: 26s - loss: 6.8291 - categorical_accuracy: 0.026 - ETA: 25s - loss: 6.8258 - categorical_accuracy: 0.028 - ETA: 24s - loss: 6.8180 - categorical_accuracy: 0.029 - ETA: 23s - loss: 6.8171 - categorical_accuracy: 0.029 - ETA: 22s - loss: 6.8171 - categorical_accuracy: 0.031 - ETA: 21s - loss: 6.8176 - categorical_accuracy: 0.031 - ETA: 21s - loss: 6.8166 - categorical_accuracy: 0.030 - ETA: 20s - loss: 6.8188 - categorical_accuracy: 0.029 - ETA: 19s - loss: 6.8196 - categorical_accuracy: 0.030 - ETA: 18s - loss: 6.8216 - categorical_accuracy: 0.031 - ETA: 18s - loss: 6.8213 - categorical_accuracy: 0.032 - ETA: 17s - loss: 6.8213 - categorical_accuracy: 0.033 - ETA: 16s - loss: 6.8214 - categorical_accuracy: 0.034 - ETA: 16s - loss: 6.8213 - categorical_accuracy: 0.034 - ETA: 15s - loss: 6.8223 - categorical_accuracy: 0.035 - ETA: 15s - loss: 6.8228 - categorical_accuracy: 0.034 - ETA: 14s - loss: 6.8218 - categorical_accuracy: 0.035 - ETA: 14s - loss: 6.8233 - categorical_accuracy: 0.036 - ETA: 13s - loss: 6.8238 - categorical_accuracy: 0.036 - ETA: 13s - loss: 6.8239 - categorical_accuracy: 0.036 - ETA: 12s - loss: 6.8248 - categorical_accuracy: 0.036 - ETA: 12s - loss: 6.8238 - categorical_accuracy: 0.035 - ETA: 11s - loss: 6.8232 - categorical_accuracy: 0.037 - ETA: 11s - loss: 6.8234 - categorical_accuracy: 0.036 - ETA: 11s - loss: 6.8235 - categorical_accuracy: 0.036 - ETA: 10s - loss: 6.8234 - categorical_accuracy: 0.036 - ETA: 10s - loss: 6.8231 - categorical_accuracy: 0.035 - ETA: 10s - loss: 6.8215 - categorical_accuracy: 0.036 - ETA: 9s - loss: 6.8207 - categorical_accuracy: 0.036 - ETA: 9s - loss: 6.8189 - categorical_accuracy: 0.03 - ETA: 8s - loss: 6.8183 - categorical_accuracy: 0.03 - ETA: 8s - loss: 6.8151 - categorical_accuracy: 0.03 - ETA: 8s - loss: 6.8126 - categorical_accuracy: 0.03 - ETA: 8s - loss: 6.8102 - categorical_accuracy: 0.03 - ETA: 7s - loss: 6.8079 - categorical_accuracy: 0.03 - ETA: 7s - loss: 6.8055 - categorical_accuracy: 0.03 - ETA: 7s - loss: 6.8019 - categorical_accuracy: 0.03 - ETA: 6s - loss: 6.7986 - categorical_accuracy: 0.03 - ETA: 6s - loss: 6.7940 - categorical_accuracy: 0.03 - ETA: 6s - loss: 6.7873 - categorical_accuracy: 0.03 - ETA: 5s - loss: 6.7801 - categorical_accuracy: 0.03 - ETA: 5s - loss: 6.7675 - categorical_accuracy: 0.03 - ETA: 5s - loss: 6.7583 - categorical_accuracy: 0.03 - ETA: 5s - loss: 6.7467 - categorical_accuracy: 0.03 - ETA: 4s - loss: 6.7374 - categorical_accuracy: 0.03 - ETA: 4s - loss: 6.7251 - categorical_accuracy: 0.03 - ETA: 4s - loss: 6.7132 - categorical_accuracy: 0.03 - ETA: 4s - loss: 6.7069 - categorical_accuracy: 0.03 - ETA: 3s - loss: 6.6937 - categorical_accuracy: 0.03 - ETA: 3s - loss: 6.6884 - categorical_accuracy: 0.03 - ETA: 3s - loss: 6.6669 - categorical_accuracy: 0.03 - ETA: 3s - loss: 6.6616 - categorical_accuracy: 0.03 - ETA: 2s - loss: 6.6574 - categorical_accuracy: 0.03 - ETA: 2s - loss: 6.6491 - categorical_accuracy: 0.03 - ETA: 2s - loss: 6.6414 - categorical_accuracy: 0.03 - ETA: 2s - loss: 6.6326 - categorical_accuracy: 0.03 - ETA: 1s - loss: 6.6192 - categorical_accuracy: 0.03 - ETA: 1s - loss: 6.6118 - categorical_accuracy: 0.03 - ETA: 1s - loss: 6.6006 - categorical_accuracy: 0.03 - ETA: 1s - loss: 6.5952 - categorical_accuracy: 0.03 - ETA: 1s - loss: 6.5873 - categorical_accuracy: 0.03 - ETA: 0s - loss: 6.5773 - categorical_accuracy: 0.03 - ETA: 0s - loss: 6.5617 - categorical_accuracy: 0.03 - ETA: 0s - loss: 6.5535 - categorical_accuracy: 0.03 - ETA: 0s - loss: 6.5450 - categorical_accuracy: 0.03 - 23s 8ms/step - loss: 6.5374 - categorical_accuracy: 0.0369 - val_loss: 5.8567 - val_categorical_accuracy: 0.0394\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2740/2740 [==============================] - ETA: 14s - loss: 6.0597 - categorical_accuracy: 0.0000e+0 - ETA: 14s - loss: 5.7408 - categorical_accuracy: 0.0000e+0 - ETA: 13s - loss: 5.6192 - categorical_accuracy: 0.0104    - ETA: 13s - loss: 5.6468 - categorical_accuracy: 0.015 - ETA: 13s - loss: 5.5979 - categorical_accuracy: 0.025 - ETA: 12s - loss: 5.5508 - categorical_accuracy: 0.026 - ETA: 12s - loss: 5.4981 - categorical_accuracy: 0.044 - ETA: 12s - loss: 5.5374 - categorical_accuracy: 0.043 - ETA: 11s - loss: 5.5803 - categorical_accuracy: 0.041 - ETA: 11s - loss: 5.5834 - categorical_accuracy: 0.043 - ETA: 11s - loss: 5.6018 - categorical_accuracy: 0.045 - ETA: 11s - loss: 5.5645 - categorical_accuracy: 0.044 - ETA: 10s - loss: 5.5876 - categorical_accuracy: 0.043 - ETA: 10s - loss: 5.5851 - categorical_accuracy: 0.044 - ETA: 10s - loss: 5.5640 - categorical_accuracy: 0.041 - ETA: 10s - loss: 5.5401 - categorical_accuracy: 0.044 - ETA: 10s - loss: 5.5301 - categorical_accuracy: 0.044 - ETA: 10s - loss: 5.5230 - categorical_accuracy: 0.046 - ETA: 9s - loss: 5.5305 - categorical_accuracy: 0.046 - ETA: 9s - loss: 5.5476 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.5671 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.5353 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.5205 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.5267 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.5233 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.5130 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.5135 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.5132 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.5221 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.5319 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.5331 - categorical_accuracy: 0.03 - ETA: 7s - loss: 5.5221 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.5179 - categorical_accuracy: 0.03 - ETA: 7s - loss: 5.5192 - categorical_accuracy: 0.03 - ETA: 7s - loss: 5.5157 - categorical_accuracy: 0.03 - ETA: 7s - loss: 5.5269 - categorical_accuracy: 0.03 - ETA: 7s - loss: 5.5291 - categorical_accuracy: 0.03 - ETA: 7s - loss: 5.5225 - categorical_accuracy: 0.03 - ETA: 6s - loss: 5.5258 - categorical_accuracy: 0.03 - ETA: 6s - loss: 5.5294 - categorical_accuracy: 0.03 - ETA: 6s - loss: 5.5153 - categorical_accuracy: 0.03 - ETA: 6s - loss: 5.5157 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.5148 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.5234 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.5321 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.5385 - categorical_accuracy: 0.03 - ETA: 5s - loss: 5.5371 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.5319 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.5395 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.5491 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.5397 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.5475 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.5466 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.5489 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.5399 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.5413 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.5348 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.5349 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.5424 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.5428 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.5428 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.5379 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.5387 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.5450 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.5496 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.5446 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.5365 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.5295 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.5291 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.5289 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.5299 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.5339 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.5275 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.5338 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.5299 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.5339 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.5359 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.5373 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.5335 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.5354 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.5371 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.5423 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.5427 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.5496 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.5530 - categorical_accuracy: 0.04 - 16s 6ms/step - loss: 5.5533 - categorical_accuracy: 0.0453 - val_loss: 5.8624 - val_categorical_accuracy: 0.0536\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2740/2740 [==============================] - ETA: 11s - loss: 5.1886 - categorical_accuracy: 0.031 - ETA: 11s - loss: 5.0024 - categorical_accuracy: 0.062 - ETA: 11s - loss: 5.1040 - categorical_accuracy: 0.062 - ETA: 11s - loss: 5.2824 - categorical_accuracy: 0.046 - ETA: 11s - loss: 5.1520 - categorical_accuracy: 0.062 - ETA: 11s - loss: 5.1985 - categorical_accuracy: 0.057 - ETA: 11s - loss: 5.3369 - categorical_accuracy: 0.053 - ETA: 11s - loss: 5.3773 - categorical_accuracy: 0.046 - ETA: 11s - loss: 5.4234 - categorical_accuracy: 0.048 - ETA: 11s - loss: 5.4281 - categorical_accuracy: 0.050 - ETA: 10s - loss: 5.4364 - categorical_accuracy: 0.048 - ETA: 10s - loss: 5.4523 - categorical_accuracy: 0.049 - ETA: 10s - loss: 5.4355 - categorical_accuracy: 0.048 - ETA: 10s - loss: 5.4595 - categorical_accuracy: 0.046 - ETA: 10s - loss: 5.4606 - categorical_accuracy: 0.047 - ETA: 10s - loss: 5.4504 - categorical_accuracy: 0.046 - ETA: 10s - loss: 5.4642 - categorical_accuracy: 0.047 - ETA: 10s - loss: 5.4406 - categorical_accuracy: 0.046 - ETA: 9s - loss: 5.4526 - categorical_accuracy: 0.046 - ETA: 9s - loss: 5.4172 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4127 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4136 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4245 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4171 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.4152 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.4180 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.4125 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.4041 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.4139 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.4164 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.4135 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4156 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4087 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4112 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4282 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4323 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4416 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4352 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4453 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4575 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4487 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4378 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4340 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4294 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4237 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4300 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4295 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4356 - categorical_accuracy: 0.05 - ETA: 5s - loss: 5.4369 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4275 - categorical_accuracy: 0.05 - ETA: 5s - loss: 5.4268 - categorical_accuracy: 0.05 - ETA: 4s - loss: 5.4302 - categorical_accuracy: 0.05 - ETA: 4s - loss: 5.4321 - categorical_accuracy: 0.05 - ETA: 4s - loss: 5.4281 - categorical_accuracy: 0.05 - ETA: 4s - loss: 5.4290 - categorical_accuracy: 0.05 - ETA: 4s - loss: 5.4251 - categorical_accuracy: 0.05 - ETA: 4s - loss: 5.4165 - categorical_accuracy: 0.05 - ETA: 4s - loss: 5.4217 - categorical_accuracy: 0.05 - ETA: 3s - loss: 5.4195 - categorical_accuracy: 0.05 - ETA: 3s - loss: 5.4197 - categorical_accuracy: 0.05 - ETA: 3s - loss: 5.4189 - categorical_accuracy: 0.05 - ETA: 3s - loss: 5.4200 - categorical_accuracy: 0.05 - ETA: 3s - loss: 5.4205 - categorical_accuracy: 0.05 - ETA: 3s - loss: 5.4323 - categorical_accuracy: 0.05 - ETA: 3s - loss: 5.4318 - categorical_accuracy: 0.05 - ETA: 2s - loss: 5.4329 - categorical_accuracy: 0.05 - ETA: 2s - loss: 5.4404 - categorical_accuracy: 0.05 - ETA: 2s - loss: 5.4403 - categorical_accuracy: 0.05 - ETA: 2s - loss: 5.4418 - categorical_accuracy: 0.05 - ETA: 2s - loss: 5.4496 - categorical_accuracy: 0.05 - ETA: 2s - loss: 5.4453 - categorical_accuracy: 0.05 - ETA: 2s - loss: 5.4508 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4519 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4497 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4534 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4534 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4570 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4597 - categorical_accuracy: 0.05 - ETA: 0s - loss: 5.4613 - categorical_accuracy: 0.05 - ETA: 0s - loss: 5.4580 - categorical_accuracy: 0.05 - ETA: 0s - loss: 5.4551 - categorical_accuracy: 0.05 - ETA: 0s - loss: 5.4612 - categorical_accuracy: 0.05 - ETA: 0s - loss: 5.4609 - categorical_accuracy: 0.05 - ETA: 0s - loss: 5.4625 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4669 - categorical_accuracy: 0.04 - 16s 6ms/step - loss: 5.4642 - categorical_accuracy: 0.0504 - val_loss: 5.9094 - val_categorical_accuracy: 0.0536\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2740/2740 [==============================] - ETA: 11s - loss: 4.8823 - categorical_accuracy: 0.0000e+0 - ETA: 12s - loss: 5.2560 - categorical_accuracy: 0.0156    - ETA: 12s - loss: 5.3875 - categorical_accuracy: 0.010 - ETA: 11s - loss: 5.2833 - categorical_accuracy: 0.046 - ETA: 11s - loss: 5.4375 - categorical_accuracy: 0.037 - ETA: 11s - loss: 5.4451 - categorical_accuracy: 0.041 - ETA: 11s - loss: 5.4549 - categorical_accuracy: 0.035 - ETA: 11s - loss: 5.3951 - categorical_accuracy: 0.054 - ETA: 11s - loss: 5.3379 - categorical_accuracy: 0.059 - ETA: 10s - loss: 5.3657 - categorical_accuracy: 0.062 - ETA: 10s - loss: 5.3542 - categorical_accuracy: 0.056 - ETA: 10s - loss: 5.3923 - categorical_accuracy: 0.052 - ETA: 10s - loss: 5.3921 - categorical_accuracy: 0.048 - ETA: 10s - loss: 5.4021 - categorical_accuracy: 0.046 - ETA: 10s - loss: 5.3912 - categorical_accuracy: 0.047 - ETA: 10s - loss: 5.4165 - categorical_accuracy: 0.044 - ETA: 10s - loss: 5.4230 - categorical_accuracy: 0.044 - ETA: 9s - loss: 5.4223 - categorical_accuracy: 0.046 - ETA: 9s - loss: 5.4100 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.3959 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4161 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4063 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.3900 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.3919 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.3710 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.3434 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.3472 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.3376 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.3352 - categorical_accuracy: 0.04 - ETA: 8s - loss: 5.3469 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3376 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3415 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3448 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3563 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3644 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3670 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3617 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3728 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3791 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3817 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3860 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3945 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3823 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3806 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3789 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3789 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3724 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3749 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3901 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3963 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.3937 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.3919 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.3975 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.3944 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.3887 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.3870 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.3845 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.3862 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.3897 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.3850 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.3912 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.3976 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4038 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4030 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.4033 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3988 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3975 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3981 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3911 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3912 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3904 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.3868 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.3906 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.3978 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.3941 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.4092 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.4125 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.4111 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4086 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4093 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4154 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4174 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4202 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4193 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4157 - categorical_accuracy: 0.04 - 16s 6ms/step - loss: 5.4150 - categorical_accuracy: 0.0445 - val_loss: 5.9583 - val_categorical_accuracy: 0.0536\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2740/2740 [==============================] - ETA: 11s - loss: 4.9629 - categorical_accuracy: 0.093 - ETA: 11s - loss: 5.0870 - categorical_accuracy: 0.062 - ETA: 11s - loss: 5.1699 - categorical_accuracy: 0.052 - ETA: 11s - loss: 5.1541 - categorical_accuracy: 0.046 - ETA: 11s - loss: 5.2420 - categorical_accuracy: 0.037 - ETA: 11s - loss: 5.2611 - categorical_accuracy: 0.046 - ETA: 11s - loss: 5.3718 - categorical_accuracy: 0.053 - ETA: 11s - loss: 5.3724 - categorical_accuracy: 0.058 - ETA: 11s - loss: 5.4291 - categorical_accuracy: 0.055 - ETA: 11s - loss: 5.4363 - categorical_accuracy: 0.053 - ETA: 11s - loss: 5.4266 - categorical_accuracy: 0.054 - ETA: 10s - loss: 5.4459 - categorical_accuracy: 0.052 - ETA: 10s - loss: 5.4512 - categorical_accuracy: 0.057 - ETA: 10s - loss: 5.4352 - categorical_accuracy: 0.058 - ETA: 10s - loss: 5.4388 - categorical_accuracy: 0.056 - ETA: 10s - loss: 5.4334 - categorical_accuracy: 0.058 - ETA: 10s - loss: 5.4238 - categorical_accuracy: 0.055 - ETA: 10s - loss: 5.4318 - categorical_accuracy: 0.055 - ETA: 9s - loss: 5.4579 - categorical_accuracy: 0.054 - ETA: 9s - loss: 5.4533 - categorical_accuracy: 0.05 - ETA: 9s - loss: 5.4637 - categorical_accuracy: 0.05 - ETA: 9s - loss: 5.4569 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4514 - categorical_accuracy: 0.04 - ETA: 9s - loss: 5.4499 - categorical_accuracy: 0.05 - ETA: 8s - loss: 5.4231 - categorical_accuracy: 0.05 - ETA: 8s - loss: 5.4209 - categorical_accuracy: 0.05 - ETA: 8s - loss: 5.4314 - categorical_accuracy: 0.05 - ETA: 8s - loss: 5.3894 - categorical_accuracy: 0.05 - ETA: 8s - loss: 5.3832 - categorical_accuracy: 0.05 - ETA: 8s - loss: 5.3673 - categorical_accuracy: 0.05 - ETA: 7s - loss: 5.3614 - categorical_accuracy: 0.05 - ETA: 7s - loss: 5.3682 - categorical_accuracy: 0.05 - ETA: 7s - loss: 5.3881 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.3952 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4066 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4175 - categorical_accuracy: 0.04 - ETA: 7s - loss: 5.4182 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4194 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4277 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4324 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4177 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4162 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.4014 - categorical_accuracy: 0.04 - ETA: 6s - loss: 5.3901 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3969 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4014 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3992 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3969 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.3989 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4074 - categorical_accuracy: 0.04 - ETA: 5s - loss: 5.4071 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.4125 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.4222 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.4185 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.4145 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.4065 - categorical_accuracy: 0.04 - ETA: 4s - loss: 5.4073 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4070 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4019 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4022 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4050 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4065 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.4063 - categorical_accuracy: 0.04 - ETA: 3s - loss: 5.3912 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3880 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3971 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.3993 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.4046 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.4064 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.4061 - categorical_accuracy: 0.04 - ETA: 2s - loss: 5.4027 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.4027 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4058 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4100 - categorical_accuracy: 0.05 - ETA: 1s - loss: 5.4136 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.4140 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.4151 - categorical_accuracy: 0.04 - ETA: 1s - loss: 5.4107 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4133 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4085 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4056 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4070 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.4001 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.3979 - categorical_accuracy: 0.04 - ETA: 0s - loss: 5.3976 - categorical_accuracy: 0.04 - 16s 6ms/step - loss: 5.3993 - categorical_accuracy: 0.0485 - val_loss: 6.1015 - val_categorical_accuracy: 0.0602\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "           ModelCheckpoint('my_model_gen_sentences_lstm.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "history = md.fit(X, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "md.save('my_model_gen_sentences_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we generate phrases, word by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocabulary...\n"
     ]
    }
   ],
   "source": [
    "#load vocabulary\n",
    "print(\"loading vocabulary...\")\n",
    "vocab_file = \"words_vocab.pkl\"\n",
    "\n",
    "with open('words_vocab.pkl', 'rb') as f:\n",
    "        words, vocab, vocabulary_inv = cPickle.load(f)\n",
    "\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# load the model\n",
    "print(\"loading model...\")\n",
    "model = load_model('my_model_gen_sentences_lstm.final.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the word generation, and tune a bit the prediction, we introduce a specific function to pick-up words.\n",
    "\n",
    "We will not take the words with the highest prediction (or the generation of text will be boring), but would like to insert some uncertainties, and let the solution sometime pick-up words with less good prediction.\n",
    "\n",
    "That is the purpose of the function **sample**, that will draw radomly a word from the vocabulary.\n",
    "\n",
    "The probabilty for a word to be drawn will depends directly on its probability to be the next word. In order to tune this probability, we introduce a \"temperature\" to smooth or sharpen its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with the following seed: \"a a a a a a a a a a a a a a a a a a a a a a a a a a a a women empowerment\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initiate sentences\n",
    "seed_sentences = \"women empowerment\"\n",
    "generated = ''\n",
    "sentence = []\n",
    "for i in range (seq_length):\n",
    "    sentence.append(\"a\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[seq_length-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "print('Generating text with the following seed: \"' + ' '.join(sentence) + '\"')\n",
    "\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a a a a a a a a a a a a a a a a a a a a a a a a women empowerment we , is we , , and of , us the the in . . , , . , we , . to . , the we , the it the the . and , we to , we we we we , . , to we the the , , and and we . we of done we . we the to , the the , that , we . . are thank of . , to women the . we . and have in the . and , , in . . the to the and we .\n"
     ]
    }
   ],
   "source": [
    "words_number = 100\n",
    "#generate the text\n",
    "for i in range(words_number):\n",
    "    #create the vector\n",
    "    x = np.zeros((1, seq_length, vocab_size))\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[0, t, vocab[word]] = 1.\n",
    "    #print(x.shape)\n",
    "\n",
    "    #calculate next word\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_index = sample(preds, 0.34)\n",
    "    next_word = vocabulary_inv[next_index]\n",
    "\n",
    "    #add the next word to the text\n",
    "    generated += \" \" + next_word\n",
    "    # shift the sentence by one, and and the next word at its end\n",
    "    sentence = sentence[1:] + [next_word]\n",
    "\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now we have to vectorize all sentences in the text, and try to find patterns in sequences of these vectors.\n",
    "\n",
    "In order to do that, we will use Doc2Vec.\n",
    "\n",
    "# 1. Doc2Vec\n",
    "Doc2Vec is able to vectorize a paragraph of text.we will transform each sentences of our text in a vector of a specific space. Doing so we will be able to compare them to retrieve the most similar sentence of a given one.\n",
    "So, once all sentences will be converted to vectors, we will try to **train a new bidirectional LSTM**. Its purpose will be to predict the best vector, next to a sequence of vectors.\n",
    "We will generate sentences as candidates to be the next phrase. We will infer their vectors using the **trained doc2Vec model**, then pick the closest one to the prediction of our new LSTM model.\n",
    "\n",
    "## 1.1 Create the Doc2Vec Model\n",
    "The first task is to create our **doc2vec model**, dedicated to our text and embedded sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tulika\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#import gensim library\n",
    "import gensim\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "#initiate sentences and labels lists\n",
    "sentences = []\n",
    "sentences_label = []\n",
    "\n",
    "#create sentences function:\n",
    "def create_sentences(doc):\n",
    "    ponctuation = [\".\",\"?\",\"!\",\":\",\"…\"]\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for word in doc:\n",
    "        if word.text not in ponctuation:\n",
    "            if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "                sent.append(word.text.lower())\n",
    "        else:\n",
    "            sent.append(word.text.lower())\n",
    "            if len(sent) > 1:\n",
    "                sentences.append(sent)\n",
    "            sent=[]\n",
    "    return sentences\n",
    "\n",
    "#create sentences from files\n",
    "input_file=\"speech.txt\"\n",
    "with codecs.open(input_file, \"r\") as f:\n",
    "    data = f.read()\n",
    "#create sentences\n",
    "doc = nlp(data)\n",
    "sents = create_sentences(doc)\n",
    "sentences = sentences + sents\n",
    "    \n",
    "#create labels\n",
    "for i in range(np.array(sentences).shape[0]):\n",
    "    sentences_label.append(\"ID\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(doc,[self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'save'\n",
    "def train_doc2vec_model(data, docLabels, size=300, sample=0.000001, dm=0, hs=1, window=10, min_count=0, workers=8,alpha=0.024, min_alpha=0.024, epoch=15, save_file='./data/doc2vec.w2v') :\n",
    "    startime = time.time()\n",
    "    \n",
    "    print(\"{0} articles loaded for model\".format(len(data)))\n",
    "\n",
    "    it = LabeledLineSentence(data, docLabels)\n",
    "\n",
    "    model = gensim.models.Doc2Vec(size=size, sample=sample, dm=dm, window=window, min_count=min_count, workers=workers,alpha=alpha, min_alpha=min_alpha, hs=hs) # use fixed learning rate\n",
    "    model.build_vocab(it)\n",
    "    for epoch in range(epoch):\n",
    "        print(\"Training epoch {}\".format(epoch + 1))\n",
    "        model.train(it,total_examples=model.corpus_count,epochs=model.iter)\n",
    "    \n",
    "        \n",
    "    #saving the created model\n",
    "    model.save(os.path.join(save_file))\n",
    "    print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 articles loaded for model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tulika\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\Users\\Tulika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tulika\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "train_doc2vec_model(sentences, sentences_label, size=500,sample=0.0,alpha=0.025, min_alpha=0.001, min_count=0, window=10, epoch=20, dm=0, hs=1, save_file='doc2vec.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : ['i', 'want', 'to', 'start', 'by', 'appreciating', 'my', 'sisters', 'and', 'brothers', 'here', 'with', 'today', ';', 'h.e.', 'ms.', 'otiko', 'afisa', 'djaba', ',', 'minister', 'for', 'gender', ',', 'children', 'and', 'social', 'protection', ',', 'ghana', 'is', 'here', ',', 'because', 'ghana', 'in', 'the', 'african', 'union', 'is', 'the', 'champion', 'for', 'gender', 'and', 'development', '.']\n",
      "***\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from six.moves import cPickle\n",
    "\n",
    "#load the model\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load('doc2vec.w2v')\n",
    "\n",
    "sentences_vector=[]\n",
    "\n",
    "t = 500\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    if i % t == 0:\n",
    "        print(\"sentence\", i, \":\", sentences[i])\n",
    "        print(\"***\")\n",
    "    sent = sentences[i]\n",
    "    sentences_vector.append(d2v_model.infer_vector(sent, alpha=0.001, min_alpha=0.001, steps=10000))\n",
    "    \n",
    "#save the sentences_vector\n",
    "sentences_vector_file =  \"sentences_vector_500_a001_ma001_s10000.pkl\"\n",
    "with open(sentences_vector_file, 'wb') as f:\n",
    "    cPickle.dump((sentences_vector), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new sequence:  0\n",
      "   1 th vector for this sequence. Sentence  ID0 (vector dim =  500 )\n",
      "   2 th vector for this sequence. Sentence  ID1 (vector dim =  500 )\n",
      "   3 th vector for this sequence. Sentence  ID2 (vector dim =  500 )\n",
      "   4 th vector for this sequence. Sentence  ID3 (vector dim =  500 )\n",
      "   5 th vector for this sequence. Sentence  ID4 (vector dim =  500 )\n",
      "   6 th vector for this sequence. Sentence  ID5 (vector dim =  500 )\n",
      "   7 th vector for this sequence. Sentence  ID6 (vector dim =  500 )\n",
      "   8 th vector for this sequence. Sentence  ID7 (vector dim =  500 )\n",
      "   9 th vector for this sequence. Sentence  ID8 (vector dim =  500 )\n",
      "   10 th vector for this sequence. Sentence  ID9 (vector dim =  500 )\n",
      "   11 th vector for this sequence. Sentence  ID10 (vector dim =  500 )\n",
      "   12 th vector for this sequence. Sentence  ID11 (vector dim =  500 )\n",
      "   13 th vector for this sequence. Sentence  ID12 (vector dim =  500 )\n",
      "   14 th vector for this sequence. Sentence  ID13 (vector dim =  500 )\n",
      "   15 th vector for this sequence. Sentence  ID14 (vector dim =  500 )\n",
      "  y vector for this sequence  ID15 : (vector dim =  500 )\n",
      "(216, 15, 500) (216, 500)\n"
     ]
    }
   ],
   "source": [
    "nb_sequenced_sentences = 15\n",
    "vector_dim = 500\n",
    "\n",
    "X_train = np.zeros((len(sentences), nb_sequenced_sentences, vector_dim), dtype=np.float)\n",
    "y_train = np.zeros((len(sentences), vector_dim), dtype=np.float)\n",
    "\n",
    "t = 1000\n",
    "for i in range(len(sentences_label)-nb_sequenced_sentences-1):\n",
    "    if i % t == 0: print(\"new sequence: \", i)\n",
    "    \n",
    "    for k in range(nb_sequenced_sentences):\n",
    "        sent = sentences_label[i+k]\n",
    "        vect = sentences_vector[i+k]\n",
    "        \n",
    "        if i % t == 0:\n",
    "            print(\"  \", k + 1 ,\"th vector for this sequence. Sentence \", sent, \"(vector dim = \", len(vect), \")\")\n",
    "            \n",
    "        for j in range(len(vect)):\n",
    "            X_train[i, k, j] = vect[j]\n",
    "    \n",
    "    senty = sentences_label[i+nb_sequenced_sentences]\n",
    "    vecty = sentences_vector[i+nb_sequenced_sentences]\n",
    "    if i % t == 0: print(\"  y vector for this sequence \", senty, \": (vector dim = \", len(vecty), \")\")\n",
    "    for j in range(len(vecty)):\n",
    "        y_train[i, j] = vecty[j]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create the Keras Model\n",
    "\n",
    "- bidirectional LSTM,\n",
    "- with size of 512 and using RELU as activation \n",
    "- then a dropout layer of 0,5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we create the model and run it to  predict the best vectorized-sentence, following a sequence of 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our process of our text generation will be : \n",
    "We have first to provide a seed of 15 sentences, that contain at least 30 words. then:\n",
    " 1. using the last 30 words of the seed, we generate 10 candidates sentences.\n",
    " 2. we infer their vectors using the doc2vec model,\n",
    " 3. we calculate the \"best vector\" for the sentence following the 15 phrases of the seed,\n",
    " 4. we compare the infered vectors with the \"best vector\", and pick-up the closest one.\n",
    " 5. we add the generated sentence corresponding to this vector at the end of the seed, as the next sentence of the text.\n",
    " 6. then, we loop over the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
